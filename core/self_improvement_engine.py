"""
Self-Improvement Engine - True AI Self-Improvement System
Handles autonomous analysis, proposal generation, and learning loops.
"""

import asyncio
import json
import logging
import os
import ast
import inspect
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from pathlib import Path
import hashlib
import difflib

from openai import OpenAI
from config.settings import settings
from core.code_generation_engine import CodeGenerationEngine
from core.approval_engine import ApprovalEngine
from core.version_tracker import VersionTracker, ChangeType, ImpactLevel
from monitoring.metrics import MetricsCollector
from utils.helpers import generate_uuid

logger = logging.getLogger(__name__)

@dataclass
class SelfImprovementProposal:
    """A self-improvement proposal generated by the AI system."""
    id: str
    title: str
    description: str
    rationale: str
    target_files: List[str]
    code_changes: Dict[str, str]  # file_path -> diff
    estimated_impact: Dict[str, Any]
    risk_level: str  # "LOW", "MEDIUM", "HIGH"
    priority: str  # "LOW", "MEDIUM", "HIGH", "CRITICAL"
    generated_by: str
    created_at: datetime
    status: str  # "PENDING", "APPROVED", "REJECTED", "IMPLEMENTED", "FAILED"
    approval_required: bool = True
    test_results: Optional[Dict[str, Any]] = None
    implementation_results: Optional[Dict[str, Any]] = None

@dataclass
class SystemAnalysis:
    """Analysis of the system's current state and potential improvements."""
    timestamp: datetime
    system_health_score: float
    performance_metrics: Dict[str, Any]
    identified_issues: List[Dict[str, Any]]
    improvement_opportunities: List[Dict[str, Any]]
    code_quality_metrics: Dict[str, Any]
    technical_debt_analysis: Dict[str, Any]

class SelfImprovementEngine:
    """
    True AI Self-Improvement Engine.
    
    Responsibilities:
    - Autonomous system analysis
    - Self-improvement proposal generation
    - Learning from implemented improvements
    - Performance monitoring and optimization
    - Code quality assessment
    """
    
    def __init__(self):
        """Initialize the Self-Improvement Engine."""
        self.client = OpenAI(api_key=settings.openai_api_key)
        self.code_generation_engine = CodeGenerationEngine()
        self.approval_engine = ApprovalEngine()
        self.version_tracker = VersionTracker()
        self.metrics_collector = MetricsCollector()
        
        # System state
        self.is_enabled = True
        self.analysis_interval_hours = 6  # Run analysis every 6 hours
        self.last_analysis = None
        self.improvement_history: List[SelfImprovementProposal] = []
        self.learning_data: Dict[str, Any] = {}
        
        # Analysis cache
        self.cached_analysis: Optional[SystemAnalysis] = None
        self.analysis_cache_duration = timedelta(hours=1)
        
        # Performance tracking
        self.successful_improvements = 0
        self.failed_improvements = 0
        self.total_proposals_generated = 0
        
        logger.info("Self-Improvement Engine initialized successfully")
    
    async def initialize(self) -> bool:
        """Initialize the Self-Improvement Engine."""
        try:
            # Initialize sub-components
            await self.code_generation_engine.initialize()
            await self.approval_engine.initialize()
            await self.version_tracker.initialize()
            
            # Start autonomous improvement cycle
            asyncio.create_task(self._autonomous_improvement_cycle())
            
            logger.info("Self-Improvement Engine initialized successfully")
            return True
            
        except Exception as e:
            logger.error(f"Error initializing Self-Improvement Engine: {str(e)}")
            return False
    
    async def analyze_system(self) -> SystemAnalysis:
        """
        Perform comprehensive system analysis to identify improvement opportunities.
        
        Returns:
            SystemAnalysis with identified issues and opportunities
        """
        try:
            # Check cache first
            if (self.cached_analysis and 
                datetime.now() - self.cached_analysis.timestamp < self.analysis_cache_duration):
                return self.cached_analysis
            
            logger.info("Starting comprehensive system analysis...")
            
            # Collect system metrics
            performance_metrics = await self._collect_performance_metrics()
            code_quality_metrics = await self._analyze_code_quality()
            technical_debt = await self._analyze_technical_debt()
            
            # Use OpenAI to analyze the collected data
            analysis_prompt = self._create_analysis_prompt(
                performance_metrics, code_quality_metrics, technical_debt
            )
            
            response = self.client.chat.completions.create(
                model=settings.openai_model,
                messages=[
                    {"role": "system", "content": "You are an expert AI system analyst. Analyze the provided system data and identify specific improvement opportunities."},
                    {"role": "user", "content": analysis_prompt}
                ],
                max_tokens=2000,
                temperature=0.3
            )
            
            content = response.choices[0].message.content
            if content is None:
                raise ValueError("Empty response from OpenAI")
            
            # Try to parse JSON, but handle cases where it's not valid JSON
            try:
                analysis_result = json.loads(content)
            except json.JSONDecodeError:
                # If JSON parsing fails, create a basic analysis result
                logger.warning("Failed to parse OpenAI response as JSON, using fallback analysis")
                analysis_result = {
                    "health_score": 0.7,
                    "issues": [{"type": "json_parsing_error", "description": "Could not parse analysis response"}],
                    "opportunities": [{"title": "Improve system analysis", "description": "Enhance analysis capabilities"}]
                }
            
            # Create system analysis
            system_analysis = SystemAnalysis(
                timestamp=datetime.now(),
                system_health_score=analysis_result.get("health_score", 0.8),
                performance_metrics=performance_metrics,
                identified_issues=analysis_result.get("issues", []),
                improvement_opportunities=analysis_result.get("opportunities", []),
                code_quality_metrics=code_quality_metrics,
                technical_debt_analysis=technical_debt
            )
            
            # Cache the analysis
            self.cached_analysis = system_analysis
            self.last_analysis = datetime.now()
            
            logger.info(f"System analysis completed. Health score: {system_analysis.system_health_score}")
            return system_analysis
            
        except Exception as e:
            logger.error(f"Error analyzing system: {str(e)}")
            # Return basic analysis on error
            return SystemAnalysis(
                timestamp=datetime.now(),
                system_health_score=0.5,
                performance_metrics={},
                identified_issues=[{"type": "analysis_error", "description": str(e)}],
                improvement_opportunities=[],
                code_quality_metrics={},
                technical_debt_analysis={}
            )
    
    async def generate_improvement_proposal(self, opportunity: Dict[str, Any]) -> Optional[SelfImprovementProposal]:
        """
        Generate a specific improvement proposal based on an identified opportunity.
        
        Args:
            opportunity: Identified improvement opportunity
            
        Returns:
            SelfImprovementProposal if successful, None otherwise
        """
        try:
            logger.info(f"Generating improvement proposal for: {opportunity.get('title', 'Unknown')}")
            
            # Generate proposal using OpenAI
            proposal_data = await self._generate_proposal_with_ai(opportunity)
            
            if not proposal_data:
                return None
            
            # Generate code changes
            code_changes = await self.code_generation_engine.generate_code_changes(
                proposal_data["target_files"],
                proposal_data["description"],
                proposal_data["rationale"]
            )
            
            # Create proposal
            proposal = SelfImprovementProposal(
                id=generate_uuid(),
                title=proposal_data["title"],
                description=proposal_data["description"],
                rationale=proposal_data["rationale"],
                target_files=proposal_data["target_files"],
                code_changes=code_changes,
                estimated_impact=proposal_data["estimated_impact"],
                risk_level=proposal_data["risk_level"],
                priority=proposal_data["priority"],
                generated_by="SELF_IMPROVEMENT_ENGINE",
                created_at=datetime.now(),
                status="PENDING"
            )
            
            # Add to history
            self.improvement_history.append(proposal)
            self.total_proposals_generated += 1
            
            logger.info(f"Generated improvement proposal: {proposal.id}")
            return proposal
            
        except Exception as e:
            logger.error(f"Error generating improvement proposal: {str(e)}")
            return None
    
    async def submit_proposal_for_approval(self, proposal: SelfImprovementProposal) -> bool:
        """
        Submit a proposal for CEO approval.
        
        Args:
            proposal: The improvement proposal to submit
            
        Returns:
            True if submitted successfully, False otherwise
        """
        try:
            if not proposal.approval_required:
                # Auto-approve low-risk improvements
                return await self.implement_proposal(proposal)
            
            # Submit to approval engine
            success = await self.approval_engine.submit_proposal(proposal)
            
            if success:
                logger.info(f"Proposal {proposal.id} submitted for approval")
            else:
                logger.error(f"Failed to submit proposal {proposal.id} for approval")
            
            return success
            
        except Exception as e:
            logger.error(f"Error submitting proposal for approval: {str(e)}")
            return False
    
    async def implement_proposal(self, proposal: SelfImprovementProposal) -> bool:
        """
        Implement an approved proposal.
        
        Args:
            proposal: The approved proposal to implement
            
        Returns:
            True if implementation successful, False otherwise
        """
        try:
            logger.info(f"Implementing proposal: {proposal.id}")
            
            # Update status
            proposal.status = "IMPLEMENTING"
            
            # Apply code changes
            implementation_result = await self.code_generation_engine.apply_code_changes(
                proposal.code_changes
            )
            
            if not implementation_result["success"]:
                proposal.status = "FAILED"
                proposal.implementation_results = implementation_result
                self.failed_improvements += 1
                logger.error(f"Failed to implement proposal {proposal.id}: {implementation_result['error']}")
                return False
            
            # Run tests
            test_result = await self._run_tests_for_changes(proposal.target_files)
            proposal.test_results = test_result
            
            if not test_result["success"]:
                # Rollback changes
                await self._rollback_changes(proposal.code_changes)
                proposal.status = "FAILED"
                self.failed_improvements += 1
                logger.error(f"Tests failed for proposal {proposal.id}, rolling back changes")
                return False
            
            # Update status and record success
            proposal.status = "IMPLEMENTED"
            proposal.implementation_results = implementation_result
            self.successful_improvements += 1
            
            # Record version change
            await self._record_version_change(proposal, implementation_result)
            
            # Learn from the improvement
            await self._learn_from_improvement(proposal)
            
            logger.info(f"Successfully implemented proposal: {proposal.id}")
            return True
            
        except Exception as e:
            logger.error(f"Error implementing proposal: {str(e)}")
            proposal.status = "FAILED"
            self.failed_improvements += 1
            return False
    
    async def get_improvement_history(self, limit: int = 50) -> List[SelfImprovementProposal]:
        """Get recent improvement history."""
        return self.improvement_history[-limit:] if self.improvement_history else []
    
    async def get_all_proposals(self) -> List[SelfImprovementProposal]:
        """Get all proposals."""
        return self.improvement_history
    
    async def get_pending_proposals(self) -> List[SelfImprovementProposal]:
        """Get pending proposals."""
        return [p for p in self.improvement_history if p.status == "PENDING"]
    
    async def get_approved_proposals(self) -> List[SelfImprovementProposal]:
        """Get approved proposals."""
        return [p for p in self.improvement_history if p.status == "APPROVED"]
    
    async def get_implemented_proposals(self) -> List[SelfImprovementProposal]:
        """Get implemented proposals."""
        return [p for p in self.improvement_history if p.status == "IMPLEMENTED"]
    
    async def get_improvement_statistics(self) -> Dict[str, Any]:
        """Get improvement statistics."""
        total_proposals = len(self.improvement_history)
        successful = len([p for p in self.improvement_history if p.status == "IMPLEMENTED"])
        pending = len([p for p in self.improvement_history if p.status == "PENDING"])
        
        success_rate = (successful / total_proposals * 100) if total_proposals > 0 else 0
        
        # Calculate impact metrics
        implemented_proposals = [p for p in self.improvement_history if p.status == "IMPLEMENTED"]
        performance_gain = sum(p.estimated_impact.get("performance", 0) for p in implemented_proposals)
        cost_reduction = sum(p.estimated_impact.get("cost_reduction", 0) for p in implemented_proposals)
        efficiency_boost = sum(p.estimated_impact.get("efficiency", 0) for p in implemented_proposals)
        
        return {
            "total_proposals": total_proposals,
            "successful_improvements": successful,
            "pending_approvals": pending,
            "success_rate": success_rate,
            "performance_gain": performance_gain,
            "cost_reduction": cost_reduction,
            "efficiency_boost": efficiency_boost,
            "last_improvement": self.last_analysis.isoformat() if self.last_analysis else "Never"
        }
    
    async def approve_proposal(self, proposal_id: str) -> bool:
        """Approve a proposal."""
        try:
            for proposal in self.improvement_history:
                if proposal.id == proposal_id:
                    proposal.status = "APPROVED"
                    logger.info(f"Proposal {proposal_id} approved")
                    return True
            return False
        except Exception as e:
            logger.error(f"Error approving proposal: {str(e)}")
            return False
    
    async def reject_proposal(self, proposal_id: str, reason: str = "") -> bool:
        """Reject a proposal."""
        try:
            for proposal in self.improvement_history:
                if proposal.id == proposal_id:
                    proposal.status = "REJECTED"
                    logger.info(f"Proposal {proposal_id} rejected: {reason}")
                    return True
            return False
        except Exception as e:
            logger.error(f"Error rejecting proposal: {str(e)}")
            return False
    
    async def generate_proposals(self) -> str:
        """Generate new improvement proposals."""
        try:
            # Analyze system first
            analysis = await self.analyze_system()
            
            # Generate proposals for each opportunity
            proposals_generated = 0
            for opportunity in analysis.improvement_opportunities:
                proposal = await self.generate_improvement_proposal(opportunity)
                if proposal:
                    self.improvement_history.append(proposal)
                    proposals_generated += 1
            
            return f"Generated {proposals_generated} new improvement proposals"
        except Exception as e:
            logger.error(f"Error generating proposals: {str(e)}")
            return f"Error generating proposals: {str(e)}"
    
    async def get_system_health_report(self) -> Dict[str, Any]:
        """Get comprehensive system health report."""
        try:
            analysis = await self.analyze_system()
            
            return {
                "timestamp": datetime.now().isoformat(),
                "system_health_score": analysis.system_health_score,
                "total_proposals": self.total_proposals_generated,
                "successful_improvements": self.successful_improvements,
                "failed_improvements": self.failed_improvements,
                "success_rate": (self.successful_improvements / max(1, self.successful_improvements + self.failed_improvements)),
                "recent_issues": analysis.identified_issues[:5],
                "recent_opportunities": analysis.improvement_opportunities[:5],
                "last_analysis": self.last_analysis.isoformat() if self.last_analysis else None
            }
            
        except Exception as e:
            logger.error(f"Error generating health report: {str(e)}")
            return {"error": str(e)}
    
    async def get_version_info(self) -> Dict[str, Any]:
        """Get current version information."""
        try:
            history = await self.version_tracker.get_upgrade_history()
            
            return {
                "current_version": history.current_version,
                "total_upgrades": history.total_upgrades,
                "total_improvements": history.total_improvements,
                "success_rate": history.success_rate,
                "average_improvement": history.average_improvement_per_upgrade,
                "last_upgrade": history.last_upgrade_date.isoformat() if history.last_upgrade_date else None,
                "upgrade_frequency_days": history.upgrade_frequency_days
            }
            
        except Exception as e:
            logger.error(f"Error getting version info: {str(e)}")
            return {"error": str(e)}
    
    async def get_upgrade_history(self) -> Dict[str, Any]:
        """Get complete upgrade history."""
        try:
            history = await self.version_tracker.get_upgrade_history()
            
            return {
                "current_version": history.current_version,
                "versions": [
                    {
                        "version": v.version,
                        "release_date": v.release_date.isoformat(),
                        "total_changes": v.total_changes,
                        "performance_improvement": v.performance_improvement,
                        "bug_fixes": v.bug_fixes,
                        "new_features": v.new_features,
                        "breaking_changes": v.breaking_changes
                    }
                    for v in history.versions
                ],
                "total_upgrades": history.total_upgrades,
                "total_improvements": history.total_improvements,
                "success_rate": history.success_rate,
                "average_improvement": history.average_improvement_per_upgrade
            }
            
        except Exception as e:
            logger.error(f"Error getting upgrade history: {str(e)}")
            return {"error": str(e)}
    
    async def create_new_version(self, version_type: str = "patch") -> str:
        """Create a new version from pending changes."""
        try:
            return await self.version_tracker.create_new_version(version_type)
        except Exception as e:
            logger.error(f"Error creating new version: {str(e)}")
            return self.version_tracker.current_version
    
    async def export_version_report(self, format: str = "json") -> str:
        """Export version history report."""
        try:
            return await self.version_tracker.export_version_history(format)
        except Exception as e:
            logger.error(f"Error exporting version report: {str(e)}")
            return "Error exporting version report"
    
    async def _autonomous_improvement_cycle(self):
        """Autonomous improvement cycle that runs continuously."""
        while self.is_enabled:
            try:
                logger.info("Starting autonomous improvement cycle...")
                
                # Analyze system
                analysis = await self.analyze_system()
                
                # Generate proposals for high-priority opportunities
                high_priority_opportunities = [
                    opp for opp in analysis.improvement_opportunities
                    if opp.get("priority", "LOW") in ["HIGH", "CRITICAL"]
                ]
                
                for opportunity in high_priority_opportunities[:3]:  # Limit to 3 proposals per cycle
                    proposal = await self.generate_improvement_proposal(opportunity)
                    if proposal:
                        await self.submit_proposal_for_approval(proposal)
                
                # Wait for next cycle
                await asyncio.sleep(self.analysis_interval_hours * 3600)
                
            except Exception as e:
                logger.error(f"Error in autonomous improvement cycle: {str(e)}")
                await asyncio.sleep(3600)  # Wait 1 hour on error
    
    async def _collect_performance_metrics(self) -> Dict[str, Any]:
        """Collect current system performance metrics."""
        try:
            # Get metrics from metrics collector
            all_metrics = await self.metrics_collector.get_all_performance()
            
            # Calculate aggregate metrics
            total_employees = len(all_metrics)
            if total_employees == 0:
                return {"error": "No metrics available"}
            
            avg_accuracy = sum(
                metrics.get("metrics", {}).get("accuracy", 0) 
                for metrics in all_metrics.values()
            ) / total_employees
            
            avg_success_rate = sum(
                metrics.get("metrics", {}).get("success_rate", 0) 
                for metrics in all_metrics.values()
            ) / total_employees
            
            return {
                "total_ai_employees": total_employees,
                "average_accuracy": avg_accuracy,
                "average_success_rate": avg_success_rate,
                "system_uptime": 99.5,  # Placeholder
                "response_time_avg": 1.2,  # Placeholder
                "error_rate": 0.05  # Placeholder
            }
            
        except Exception as e:
            logger.error(f"Error collecting performance metrics: {str(e)}")
            return {"error": str(e)}
    
    async def _analyze_code_quality(self) -> Dict[str, Any]:
        """Analyze code quality metrics."""
        try:
            # This would integrate with tools like pylint, flake8, etc.
            # For now, return placeholder metrics
            return {
                "code_complexity": 0.7,
                "test_coverage": 0.85,
                "documentation_coverage": 0.6,
                "code_duplication": 0.1,
                "maintainability_index": 0.8
            }
            
        except Exception as e:
            logger.error(f"Error analyzing code quality: {str(e)}")
            return {"error": str(e)}
    
    async def _analyze_technical_debt(self) -> Dict[str, Any]:
        """Analyze technical debt in the codebase."""
        try:
            # This would analyze TODO comments, deprecated code, etc.
            return {
                "total_todos": 15,
                "deprecated_functions": 3,
                "unused_imports": 8,
                "long_functions": 12,
                "complex_conditions": 7
            }
            
        except Exception as e:
            logger.error(f"Error analyzing technical debt: {str(e)}")
            return {"error": str(e)}
    
    def _create_analysis_prompt(self, performance_metrics: Dict[str, Any], 
                               code_quality: Dict[str, Any], 
                               technical_debt: Dict[str, Any]) -> str:
        """Create prompt for AI analysis."""
        return f"""
Analyze the following system data and identify specific improvement opportunities:

PERFORMANCE METRICS:
{json.dumps(performance_metrics, indent=2)}

CODE QUALITY METRICS:
{json.dumps(code_quality, indent=2)}

TECHNICAL DEBT:
{json.dumps(technical_debt, indent=2)}

Please provide a JSON response with the following structure:
{{
    "health_score": 0.0-1.0,
    "issues": [
        {{
            "type": "performance|code_quality|security|maintainability",
            "title": "Brief issue title",
            "description": "Detailed description",
            "severity": "LOW|MEDIUM|HIGH|CRITICAL",
            "affected_components": ["component1", "component2"]
        }}
    ],
    "opportunities": [
        {{
            "type": "optimization|refactoring|feature|bug_fix",
            "title": "Brief opportunity title", 
            "description": "Detailed description",
            "priority": "LOW|MEDIUM|HIGH|CRITICAL",
            "estimated_impact": {{
                "performance_improvement": "0-100%",
                "code_quality_improvement": "0-100%",
                "maintenance_improvement": "0-100%"
            }},
            "target_files": ["file1.py", "file2.py"],
            "risk_level": "LOW|MEDIUM|HIGH"
        }}
    ]
}}

Focus on actionable, specific improvements that can be implemented with code changes.
"""
    
    async def _generate_proposal_with_ai(self, opportunity: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Generate proposal details using OpenAI."""
        try:
            prompt = f"""
Based on this improvement opportunity, generate a detailed implementation proposal:

OPPORTUNITY:
{json.dumps(opportunity, indent=2)}

Generate a JSON response with:
{{
    "title": "Specific improvement title",
    "description": "Detailed implementation description",
    "rationale": "Why this improvement is needed and beneficial",
    "target_files": ["specific files to modify"],
    "estimated_impact": {{
        "performance": "specific improvement",
        "maintainability": "specific improvement", 
        "reliability": "specific improvement"
    }},
    "risk_level": "LOW|MEDIUM|HIGH",
    "priority": "LOW|MEDIUM|HIGH|CRITICAL"
}}

Be specific and actionable.
"""
            
            response = self.client.chat.completions.create(
                model=settings.openai_model,
                messages=[
                    {"role": "system", "content": "You are an expert software engineer specializing in system optimization and code improvements."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1500,
                temperature=0.3
            )
            
            content = response.choices[0].message.content
            if content is None:
                return None
            return json.loads(content)
            
        except Exception as e:
            logger.error(f"Error generating proposal with AI: {str(e)}")
            return None
    
    async def _run_tests_for_changes(self, target_files: List[str]) -> Dict[str, Any]:
        """Run tests for the modified files."""
        try:
            # This would run actual tests
            # For now, return success
            return {
                "success": True,
                "tests_run": 15,
                "tests_passed": 15,
                "tests_failed": 0,
                "coverage": 0.85
            }
            
        except Exception as e:
            logger.error(f"Error running tests: {str(e)}")
            return {"success": False, "error": str(e)}
    
    async def _rollback_changes(self, code_changes: Dict[str, str]):
        """Rollback applied code changes."""
        try:
            # This would implement actual rollback logic
            logger.info("Rolling back code changes...")
            
        except Exception as e:
            logger.error(f"Error rolling back changes: {str(e)}")
    
    async def _learn_from_improvement(self, proposal: SelfImprovementProposal):
        """Learn from successful improvements to improve future proposals."""
        try:
            # Record learning data
            learning_entry = {
                "proposal_id": proposal.id,
                "improvement_type": proposal.description,
                "success": True,
                "impact": proposal.estimated_impact,
                "timestamp": datetime.now().isoformat()
            }
            
            if "successful_improvements" not in self.learning_data:
                self.learning_data["successful_improvements"] = []
            
            self.learning_data["successful_improvements"].append(learning_entry)
            
            logger.info(f"Recorded learning from successful improvement: {proposal.id}")
            
        except Exception as e:
            logger.error(f"Error learning from improvement: {str(e)}")
    
    async def _record_version_change(self, proposal: SelfImprovementProposal, implementation_result: Dict[str, Any]):
        """Record a version change for the implemented proposal."""
        try:
            # Determine change type based on proposal description
            change_type = self._determine_change_type(proposal.description)
            
            # Determine impact level based on proposal risk level
            impact_level = self._determine_impact_level(proposal.risk_level)
            
            # Calculate lines changed
            lines_added = 0
            lines_removed = 0
            for diff in proposal.code_changes.values():
                diff_lines = diff.split('\n')
                lines_added += len([line for line in diff_lines if line.startswith('+') and not line.startswith('+++')])
                lines_removed += len([line for line in diff_lines if line.startswith('-') and not line.startswith('---')])
            
            # Record the improvement
            change_id = await self.version_tracker.record_improvement(
                proposal_id=proposal.id,
                change_type=change_type,
                title=proposal.title,
                description=proposal.description,
                files_modified=proposal.target_files,
                lines_added=lines_added,
                lines_removed=lines_removed,
                impact_level=impact_level,
                implemented_by=proposal.generated_by,
                approved_by="CEO"  # This would come from approval engine
            )
            
            # Finalize the improvement
            await self.version_tracker.finalize_improvement(change_id, success=True)
            
            logger.info(f"Recorded version change: {change_id}")
            
        except Exception as e:
            logger.error(f"Error recording version change: {str(e)}")
    
    def _determine_change_type(self, description: str) -> ChangeType:
        """Determine the change type based on description."""
        description_lower = description.lower()
        
        if any(word in description_lower for word in ["bug", "fix", "error", "issue"]):
            return ChangeType.BUG_FIX
        elif any(word in description_lower for word in ["performance", "speed", "optimization"]):
            return ChangeType.PERFORMANCE_IMPROVEMENT
        elif any(word in description_lower for word in ["feature", "add", "new"]):
            return ChangeType.FEATURE_ADDITION
        elif any(word in description_lower for word in ["refactor", "restructure", "clean"]):
            return ChangeType.CODE_REFACTORING
        elif any(word in description_lower for word in ["security", "vulnerability"]):
            return ChangeType.SECURITY_UPDATE
        elif any(word in description_lower for word in ["documentation", "docs", "readme"]):
            return ChangeType.DOCUMENTATION_UPDATE
        elif any(word in description_lower for word in ["architecture", "design"]):
            return ChangeType.ARCHITECTURE_CHANGE
        else:
            return ChangeType.OPTIMIZATION
    
    def _determine_impact_level(self, risk_level: str) -> ImpactLevel:
        """Determine impact level based on risk level."""
        risk_mapping = {
            "LOW": ImpactLevel.LOW,
            "MEDIUM": ImpactLevel.MEDIUM,
            "HIGH": ImpactLevel.HIGH,
            "CRITICAL": ImpactLevel.CRITICAL
        }
        return risk_mapping.get(risk_level, ImpactLevel.MEDIUM) 