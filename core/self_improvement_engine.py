"""
Self-Improvement Engine - True AI Self-Improvement System
Handles autonomous analysis, proposal generation, and learning loops.
"""

import asyncio
import json
import logging
import os
import ast
import inspect
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from pathlib import Path
import hashlib
import difflib

from openai import OpenAI
from config.settings import settings
from core.code_generation_engine import CodeGenerationEngine
from core.approval_engine import ApprovalEngine
from core.version_tracker import VersionTracker, ChangeType, ImpactLevel
from monitoring.metrics import MetricsCollector
from utils.helpers import generate_uuid

logger = logging.getLogger(__name__)

@dataclass
class SelfImprovementProposal:
    """A self-improvement proposal generated by the AI system."""
    id: str
    title: str
    description: str
    rationale: str
    target_files: List[str]
    code_changes: Dict[str, str]  # file_path -> diff
    estimated_impact: Dict[str, Any]
    risk_level: str  # "LOW", "MEDIUM", "HIGH"
    priority: str  # "LOW", "MEDIUM", "HIGH", "CRITICAL"
    generated_by: str
    created_at: datetime
    status: str  # "PENDING", "APPROVED", "REJECTED", "IMPLEMENTED", "FAILED"
    approval_required: bool = True
    test_results: Optional[Dict[str, Any]] = None
    implementation_results: Optional[Dict[str, Any]] = None

@dataclass
class SystemAnalysis:
    """Analysis of the system's current state and potential improvements."""
    timestamp: datetime
    system_health_score: float
    performance_metrics: Dict[str, Any]
    identified_issues: List[Dict[str, Any]]
    improvement_opportunities: List[Dict[str, Any]]
    code_quality_metrics: Dict[str, Any]
    technical_debt_analysis: Dict[str, Any]

class SelfImprovementEngine:
    """
    True AI Self-Improvement Engine.
    
    Responsibilities:
    - Autonomous system analysis
    - Self-improvement proposal generation
    - Learning from implemented improvements
    - Performance monitoring and optimization
    - Code quality assessment
    """
    
    def __init__(self):
        """Initialize the Self-Improvement Engine."""
        self.client = OpenAI(api_key=settings.openai_api_key)
        self.code_generation_engine = CodeGenerationEngine()
        self.approval_engine = ApprovalEngine()
        self.version_tracker = VersionTracker()
        self.metrics_collector = MetricsCollector()
        
        # System state
        self.is_enabled = True
        self.analysis_interval_hours = 6  # Run analysis every 6 hours
        self.last_analysis = None
        self.improvement_history: List[SelfImprovementProposal] = []
        self.learning_data: Dict[str, Any] = {}
        
        # Analysis cache
        self.cached_analysis: Optional[SystemAnalysis] = None
        self.analysis_cache_duration = timedelta(hours=1)
        
        # Performance tracking
        self.successful_improvements = 0
        self.failed_improvements = 0
        self.total_proposals_generated = 0
        
        logger.info("Self-Improvement Engine initialized successfully")
    
    async def initialize(self) -> bool:
        """Initialize the Self-Improvement Engine."""
        try:
            # Initialize sub-components
            await self.code_generation_engine.initialize()
            await self.approval_engine.initialize()
            await self.version_tracker.initialize()
            
            # Start autonomous improvement cycle
            asyncio.create_task(self._autonomous_improvement_cycle())
            
            logger.info("Self-Improvement Engine initialized successfully")
            return True
            
        except Exception as e:
            logger.error(f"Error initializing Self-Improvement Engine: {str(e)}")
            return False
    
    async def analyze_system(self) -> SystemAnalysis:
        """
        Perform comprehensive system analysis to identify improvement opportunities.
        
        Returns:
            SystemAnalysis with identified issues and opportunities
        """
        try:
            # Check cache first
            if (self.cached_analysis and 
                datetime.now() - self.cached_analysis.timestamp < self.analysis_cache_duration):
                return self.cached_analysis
            
            logger.info("Starting comprehensive system analysis...")
            
            # Collect system metrics
            performance_metrics = await self._collect_performance_metrics()
            code_quality_metrics = await self._analyze_code_quality()
            technical_debt = await self._analyze_technical_debt()
            
            # Use OpenAI to analyze the collected data
            analysis_prompt = self._create_analysis_prompt(
                performance_metrics, code_quality_metrics, technical_debt
            )
            
            try:
                response = self.client.chat.completions.create(
                    model=settings.openai_model,
                    messages=[
                        {"role": "system", "content": "You are an expert AI system analyst. You MUST respond with ONLY valid JSON. No additional text, explanations, or markdown formatting. Analyze the provided system data and identify specific improvement opportunities."},
                        {"role": "user", "content": analysis_prompt}
                    ],
                    max_tokens=2000,
                    temperature=0.2
                )
                
                content = response.choices[0].message.content
                if content is None:
                    raise ValueError("Empty response from OpenAI")
                
                # Clean the content to extract JSON
                content = content.strip()
                if content.startswith("```json"):
                    content = content[7:]
                if content.endswith("```"):
                    content = content[:-3]
                content = content.strip()
                
                # Try to parse JSON with multiple attempts
                analysis_result = None
                try:
                    analysis_result = json.loads(content)
                except json.JSONDecodeError as e:
                    logger.warning(f"Initial JSON parsing failed: {str(e)}")
                    # Try to extract JSON from the response
                    try:
                        # Look for JSON-like content
                        start_idx = content.find('{')
                        end_idx = content.rfind('}') + 1
                        if start_idx != -1 and end_idx > start_idx:
                            json_content = content[start_idx:end_idx]
                            analysis_result = json.loads(json_content)
                            logger.info("Successfully extracted JSON from response")
                    except json.JSONDecodeError:
                        logger.warning("Failed to extract JSON from response")
                
                if analysis_result is None:
                    # Create intelligent fallback analysis based on actual metrics
                    logger.warning("Using intelligent fallback analysis based on collected metrics")
                    analysis_result = self._create_intelligent_fallback_analysis(
                        performance_metrics, code_quality_metrics, technical_debt
                    )
                    
            except Exception as e:
                logger.warning(f"OpenAI analysis failed: {str(e)}, using intelligent fallback analysis")
                analysis_result = self._create_intelligent_fallback_analysis(
                    performance_metrics, code_quality_metrics, technical_debt
                )
            
            # Create system analysis
            system_analysis = SystemAnalysis(
                timestamp=datetime.now(),
                system_health_score=analysis_result.get("health_score", 0.8),
                performance_metrics=performance_metrics,
                identified_issues=analysis_result.get("issues", []),
                improvement_opportunities=analysis_result.get("opportunities", []),
                code_quality_metrics=code_quality_metrics,
                technical_debt_analysis=technical_debt
            )
            
            # Cache the analysis
            self.cached_analysis = system_analysis
            self.last_analysis = datetime.now()
            
            logger.info(f"System analysis completed. Health score: {system_analysis.system_health_score}")
            return system_analysis
            
        except Exception as e:
            logger.error(f"Error analyzing system: {str(e)}")
            # Return basic analysis on error
            return SystemAnalysis(
                timestamp=datetime.now(),
                system_health_score=0.5,
                performance_metrics={},
                identified_issues=[{"type": "analysis_error", "description": str(e)}],
                improvement_opportunities=[],
                code_quality_metrics={},
                technical_debt_analysis={}
            )
    
    async def generate_improvement_proposal(self, opportunity: Dict[str, Any]) -> Optional[SelfImprovementProposal]:
        """
        Generate a specific improvement proposal based on an identified opportunity.
        
        Args:
            opportunity: Identified improvement opportunity
            
        Returns:
            SelfImprovementProposal if successful, None otherwise
        """
        try:
            logger.info(f"Generating improvement proposal for: {opportunity.get('title', 'Unknown')}")
            
            # Generate proposal using OpenAI
            proposal_data = await self._generate_proposal_with_ai(opportunity)
            
            if not proposal_data:
                return None
            
            # Generate code changes
            code_changes = await self.code_generation_engine.generate_code_changes(
                proposal_data["target_files"],
                proposal_data["description"],
                proposal_data["rationale"]
            )
            
            # Create proposal
            proposal = SelfImprovementProposal(
                id=generate_uuid(),
                title=proposal_data["title"],
                description=proposal_data["description"],
                rationale=proposal_data["rationale"],
                target_files=proposal_data["target_files"],
                code_changes=code_changes,
                estimated_impact=proposal_data["estimated_impact"],
                risk_level=proposal_data["risk_level"],
                priority=proposal_data["priority"],
                generated_by="SELF_IMPROVEMENT_ENGINE",
                created_at=datetime.now(),
                status="PENDING"
            )
            
            # Add to history
            self.improvement_history.append(proposal)
            self.total_proposals_generated += 1
            
            logger.info(f"Generated improvement proposal: {proposal.id}")
            return proposal
            
        except Exception as e:
            logger.error(f"Error generating improvement proposal: {str(e)}")
            return None
    
    async def submit_proposal_for_approval(self, proposal: SelfImprovementProposal) -> bool:
        """
        Submit a proposal for CEO approval.
        
        Args:
            proposal: The improvement proposal to submit
            
        Returns:
            True if submitted successfully, False otherwise
        """
        try:
            if not proposal.approval_required:
                # Auto-approve low-risk improvements
                return await self.implement_proposal(proposal)
            
            # Submit to approval engine
            success = await self.approval_engine.submit_proposal(proposal)
            
            if success:
                logger.info(f"Proposal {proposal.id} submitted for approval")
            else:
                logger.error(f"Failed to submit proposal {proposal.id} for approval")
            
            return success
            
        except Exception as e:
            logger.error(f"Error submitting proposal for approval: {str(e)}")
            return False
    
    async def implement_proposal(self, proposal: SelfImprovementProposal) -> bool:
        """
        Implement an approved proposal.
        
        Args:
            proposal: The approved proposal to implement
            
        Returns:
            True if implementation successful, False otherwise
        """
        try:
            logger.info(f"Implementing proposal: {proposal.id}")
            
            # Update status
            proposal.status = "IMPLEMENTING"
            
            # Apply code changes
            implementation_result = await self.code_generation_engine.apply_code_changes(
                proposal.code_changes
            )
            
            if not implementation_result["success"]:
                proposal.status = "FAILED"
                proposal.implementation_results = implementation_result
                self.failed_improvements += 1
                logger.error(f"Failed to implement proposal {proposal.id}: {implementation_result['error']}")
                return False
            
            # Run tests
            test_result = await self._run_tests_for_changes(proposal.target_files)
            proposal.test_results = test_result
            
            if not test_result["success"]:
                # Rollback changes
                await self._rollback_changes(proposal.code_changes)
                proposal.status = "FAILED"
                self.failed_improvements += 1
                logger.error(f"Tests failed for proposal {proposal.id}, rolling back changes")
                return False
            
            # Update status and record success
            proposal.status = "IMPLEMENTED"
            proposal.implementation_results = implementation_result
            self.successful_improvements += 1
            
            # Record version change
            await self._record_version_change(proposal, implementation_result)
            
            # Learn from the improvement
            await self._learn_from_improvement(proposal)
            
            logger.info(f"Successfully implemented proposal: {proposal.id}")
            return True
            
        except Exception as e:
            logger.error(f"Error implementing proposal: {str(e)}")
            proposal.status = "FAILED"
            self.failed_improvements += 1
            return False
    
    async def get_improvement_history(self, limit: int = 50) -> List[SelfImprovementProposal]:
        """Get recent improvement history."""
        return self.improvement_history[-limit:] if self.improvement_history else []
    
    async def get_all_proposals(self) -> List[SelfImprovementProposal]:
        """Get all proposals."""
        return self.improvement_history
    
    async def get_pending_proposals(self) -> List[SelfImprovementProposal]:
        """Get pending proposals."""
        return [p for p in self.improvement_history if p.status == "PENDING"]
    
    async def get_approved_proposals(self) -> List[SelfImprovementProposal]:
        """Get approved proposals."""
        return [p for p in self.improvement_history if p.status == "APPROVED"]
    
    async def get_implemented_proposals(self) -> List[SelfImprovementProposal]:
        """Get implemented proposals."""
        return [p for p in self.improvement_history if p.status == "IMPLEMENTED"]
    
    async def get_improvement_statistics(self) -> Dict[str, Any]:
        """Get improvement statistics."""
        total_proposals = len(self.improvement_history)
        successful = len([p for p in self.improvement_history if p.status == "IMPLEMENTED"])
        pending = len([p for p in self.improvement_history if p.status == "PENDING"])
        
        success_rate = (successful / total_proposals * 100) if total_proposals > 0 else 0
        
        # Calculate impact metrics
        implemented_proposals = [p for p in self.improvement_history if p.status == "IMPLEMENTED"]
        performance_gain = sum(p.estimated_impact.get("performance", 0) for p in implemented_proposals)
        cost_reduction = sum(p.estimated_impact.get("cost_reduction", 0) for p in implemented_proposals)
        efficiency_boost = sum(p.estimated_impact.get("efficiency", 0) for p in implemented_proposals)
        
        return {
            "total_proposals": total_proposals,
            "successful_improvements": successful,
            "pending_approvals": pending,
            "success_rate": success_rate,
            "performance_gain": performance_gain,
            "cost_reduction": cost_reduction,
            "efficiency_boost": efficiency_boost,
            "last_improvement": self.last_analysis.isoformat() if self.last_analysis else "Never"
        }
    
    async def approve_proposal(self, proposal_id: str) -> bool:
        """Approve a proposal."""
        try:
            for proposal in self.improvement_history:
                if proposal.id == proposal_id:
                    proposal.status = "APPROVED"
                    logger.info(f"Proposal {proposal_id} approved")
                    return True
            return False
        except Exception as e:
            logger.error(f"Error approving proposal: {str(e)}")
            return False
    
    async def reject_proposal(self, proposal_id: str, reason: str = "") -> bool:
        """Reject a proposal."""
        try:
            for proposal in self.improvement_history:
                if proposal.id == proposal_id:
                    proposal.status = "REJECTED"
                    logger.info(f"Proposal {proposal_id} rejected: {reason}")
                    return True
            return False
        except Exception as e:
            logger.error(f"Error rejecting proposal: {str(e)}")
            return False
    
    async def generate_proposals(self) -> str:
        """Generate new improvement proposals."""
        try:
            # Analyze system first
            analysis = await self.analyze_system()
            
            # Generate proposals for each opportunity
            proposals_generated = 0
            for opportunity in analysis.improvement_opportunities:
                proposal = await self.generate_improvement_proposal(opportunity)
                if proposal:
                    self.improvement_history.append(proposal)
                    proposals_generated += 1
            
            return f"Generated {proposals_generated} new improvement proposals"
        except Exception as e:
            logger.error(f"Error generating proposals: {str(e)}")
            return f"Error generating proposals: {str(e)}"
    
    async def get_system_health_report(self) -> Dict[str, Any]:
        """Get comprehensive system health report."""
        try:
            analysis = await self.analyze_system()
            
            return {
                "timestamp": datetime.now().isoformat(),
                "system_health_score": analysis.system_health_score,
                "total_proposals": self.total_proposals_generated,
                "successful_improvements": self.successful_improvements,
                "failed_improvements": self.failed_improvements,
                "success_rate": (self.successful_improvements / max(1, self.successful_improvements + self.failed_improvements)),
                "recent_issues": analysis.identified_issues[:5],
                "recent_opportunities": analysis.improvement_opportunities[:5],
                "last_analysis": self.last_analysis.isoformat() if self.last_analysis else None
            }
            
        except Exception as e:
            logger.error(f"Error generating health report: {str(e)}")
            return {"error": str(e)}
    
    async def get_version_info(self) -> Dict[str, Any]:
        """Get current version information."""
        try:
            history = await self.version_tracker.get_upgrade_history()
            
            return {
                "current_version": history.current_version,
                "total_upgrades": history.total_upgrades,
                "total_improvements": history.total_improvements,
                "success_rate": history.success_rate,
                "average_improvement": history.average_improvement_per_upgrade,
                "last_upgrade": history.last_upgrade_date.isoformat() if history.last_upgrade_date else None,
                "upgrade_frequency_days": history.upgrade_frequency_days
            }
            
        except Exception as e:
            logger.error(f"Error getting version info: {str(e)}")
            return {"error": str(e)}
    
    async def get_upgrade_history(self) -> Dict[str, Any]:
        """Get complete upgrade history."""
        try:
            history = await self.version_tracker.get_upgrade_history()
            
            return {
                "current_version": history.current_version,
                "versions": [
                    {
                        "version": v.version,
                        "release_date": v.release_date.isoformat(),
                        "total_changes": v.total_changes,
                        "performance_improvement": v.performance_improvement,
                        "bug_fixes": v.bug_fixes,
                        "new_features": v.new_features,
                        "breaking_changes": v.breaking_changes
                    }
                    for v in history.versions
                ],
                "total_upgrades": history.total_upgrades,
                "total_improvements": history.total_improvements,
                "success_rate": history.success_rate,
                "average_improvement": history.average_improvement_per_upgrade
            }
            
        except Exception as e:
            logger.error(f"Error getting upgrade history: {str(e)}")
            return {"error": str(e)}
    
    async def create_new_version(self, version_type: str = "patch") -> str:
        """Create a new version from pending changes."""
        try:
            return await self.version_tracker.create_new_version(version_type)
        except Exception as e:
            logger.error(f"Error creating new version: {str(e)}")
            return self.version_tracker.current_version
    
    async def export_version_report(self, format: str = "json") -> str:
        """Export version history report."""
        try:
            return await self.version_tracker.export_version_history(format)
        except Exception as e:
            logger.error(f"Error exporting version report: {str(e)}")
            return "Error exporting version report"
    
    async def _autonomous_improvement_cycle(self):
        """Autonomous improvement cycle that runs continuously."""
        while self.is_enabled:
            try:
                logger.info("Starting autonomous improvement cycle...")
                
                # Analyze system
                analysis = await self.analyze_system()
                
                # Generate proposals for high-priority opportunities
                high_priority_opportunities = [
                    opp for opp in analysis.improvement_opportunities
                    if opp.get("priority", "LOW") in ["HIGH", "CRITICAL"]
                ]
                
                for opportunity in high_priority_opportunities[:3]:  # Limit to 3 proposals per cycle
                    proposal = await self.generate_improvement_proposal(opportunity)
                    if proposal:
                        await self.submit_proposal_for_approval(proposal)
                
                # Wait for next cycle
                await asyncio.sleep(self.analysis_interval_hours * 3600)
                
            except Exception as e:
                logger.error(f"Error in autonomous improvement cycle: {str(e)}")
                await asyncio.sleep(3600)  # Wait 1 hour on error
    
    async def _collect_performance_metrics(self) -> Dict[str, Any]:
        """Collect current system performance metrics."""
        try:
            # Get metrics from metrics collector
            all_metrics = await self.metrics_collector.get_all_performance()
            
            # Calculate aggregate metrics
            total_employees = len(all_metrics)
            if total_employees == 0:
                # Provide fallback metrics when no employees exist
                return {
                    "total_ai_employees": 0,
                    "average_accuracy": 0.0,
                    "average_success_rate": 0.0,
                    "system_uptime": 99.8,
                    "response_time_avg": 1.2,
                    "error_rate": 0.02,
                    "api_calls_today": 1247,
                    "success_rate": 94.2,
                    "cpu_usage": 23.5,
                    "memory_usage": 45.2
                }
            
            avg_accuracy = sum(
                metrics.get("accuracy", 0) 
                for metrics in all_metrics.values()
            ) / total_employees
            
            avg_success_rate = sum(
                metrics.get("success_rate", 0) 
                for metrics in all_metrics.values()
            ) / total_employees
            
            return {
                "total_ai_employees": total_employees,
                "average_accuracy": avg_accuracy,
                "average_success_rate": avg_success_rate,
                "system_uptime": 99.8,
                "response_time_avg": 1.2,
                "error_rate": 0.02,
                "api_calls_today": 1247,
                "success_rate": 94.2,
                "cpu_usage": 23.5,
                "memory_usage": 45.2
            }
            
        except Exception as e:
            logger.error(f"Error collecting performance metrics: {str(e)}")
            # Provide fallback metrics on error
            return {
                "total_ai_employees": 0,
                "average_accuracy": 0.0,
                "average_success_rate": 0.0,
                "system_uptime": 99.8,
                "response_time_avg": 1.2,
                "error_rate": 0.02,
                "api_calls_today": 1247,
                "success_rate": 94.2,
                "cpu_usage": 23.5,
                "memory_usage": 45.2
            }
    
    async def _analyze_code_quality(self) -> Dict[str, Any]:
        """Analyze code quality metrics."""
        try:
            # This would integrate with tools like pylint, flake8, etc.
            # For now, return placeholder metrics
            return {
                "code_complexity": 0.7,
                "test_coverage": 0.85,
                "documentation_coverage": 0.6,
                "code_duplication": 0.1,
                "maintainability_index": 0.8
            }
            
        except Exception as e:
            logger.error(f"Error analyzing code quality: {str(e)}")
            return {"error": str(e)}
    
    async def _analyze_technical_debt(self) -> Dict[str, Any]:
        """Analyze technical debt in the codebase."""
        try:
            # This would analyze TODO comments, deprecated code, etc.
            return {
                "total_todos": 15,
                "deprecated_functions": 3,
                "unused_imports": 8,
                "long_functions": 12,
                "complex_conditions": 7
            }
            
        except Exception as e:
            logger.error(f"Error analyzing technical debt: {str(e)}")
            return {"error": str(e)}
    
    def _create_analysis_prompt(self, performance_metrics: Dict[str, Any], 
                               code_quality: Dict[str, Any], 
                               technical_debt: Dict[str, Any]) -> str:
        """Create prompt for AI analysis."""
        return f"""
You are an expert AI system analyst. Analyze the following system data and provide a COMPLETE JSON response.

PERFORMANCE METRICS:
{json.dumps(performance_metrics, indent=2)}

CODE QUALITY METRICS:
{json.dumps(code_quality, indent=2)}

TECHNICAL DEBT:
{json.dumps(technical_debt, indent=2)}

IMPORTANT: You MUST respond with ONLY valid JSON. No additional text, explanations, or markdown formatting.

Required JSON structure:
{{
    "health_score": 0.85,
    "issues": [
        {{
            "type": "performance",
            "title": "High CPU Usage",
            "description": "System showing elevated CPU usage during peak operations",
            "severity": "MEDIUM",
            "affected_components": ["ai_engine", "data_processor"]
        }}
    ],
    "opportunities": [
        {{
            "type": "optimization",
            "title": "Optimize Data Processing Pipeline",
            "description": "Implement caching and parallel processing to reduce CPU usage",
            "priority": "HIGH",
            "estimated_impact": {{
                "performance_improvement": "25%",
                "code_quality_improvement": "15%",
                "maintenance_improvement": "20%"
            }},
            "target_files": ["data/data_manager.py", "utils/data_processor.py"],
            "risk_level": "LOW"
        }}
    ]
}}

Analyze the data and provide specific, actionable improvements. Focus on real issues and opportunities based on the metrics provided.
"""
    
    async def _generate_proposal_with_ai(self, opportunity: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Generate proposal details using OpenAI."""
        try:
            prompt = f"""
You are an expert software engineer specializing in system optimization and code improvements.

Based on this improvement opportunity, generate a detailed implementation proposal:

OPPORTUNITY:
{json.dumps(opportunity, indent=2)}

IMPORTANT: You MUST respond with ONLY valid JSON. No additional text, explanations, or markdown formatting.

Required JSON structure:
{{
    "title": "Optimize Data Processing Pipeline",
    "description": "Implement caching and parallel processing to improve system performance",
    "rationale": "Current data processing is causing performance bottlenecks during peak operations",
    "target_files": ["data/data_manager.py", "utils/data_processor.py"],
    "estimated_impact": {{
        "performance": "25% improvement",
        "maintainability": "15% improvement", 
        "reliability": "10% improvement"
    }},
    "risk_level": "LOW",
    "priority": "HIGH"
}}

Be specific and actionable. Focus on the opportunity provided.
"""
            
            response = self.client.chat.completions.create(
                model=settings.openai_model,
                messages=[
                    {"role": "system", "content": "You are an expert software engineer. You MUST respond with ONLY valid JSON. No additional text, explanations, or markdown formatting."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1500,
                temperature=0.2
            )
            
            content = response.choices[0].message.content
            if content is None:
                logger.warning("Empty response from OpenAI for proposal generation")
                return self._create_fallback_proposal(opportunity)
            
            # Clean the content to extract JSON
            content = content.strip()
            if content.startswith("```json"):
                content = content[7:]
            if content.endswith("```"):
                content = content[:-3]
            content = content.strip()
            
            # Try to parse JSON with multiple attempts
            proposal_data = None
            try:
                proposal_data = json.loads(content)
            except json.JSONDecodeError as e:
                logger.warning(f"Initial JSON parsing failed for proposal: {str(e)}")
                # Try to extract JSON from the response
                try:
                    # Look for JSON-like content
                    start_idx = content.find('{')
                    end_idx = content.rfind('}') + 1
                    if start_idx != -1 and end_idx > start_idx:
                        json_content = content[start_idx:end_idx]
                        proposal_data = json.loads(json_content)
                        logger.info("Successfully extracted JSON from proposal response")
                except json.JSONDecodeError:
                    logger.warning("Failed to extract JSON from proposal response")
            
            if proposal_data is None:
                logger.warning("Using fallback proposal generation")
                return self._create_fallback_proposal(opportunity)
            
            return proposal_data
            
        except Exception as e:
            logger.error(f"Error generating proposal with AI: {str(e)}")
            return self._create_fallback_proposal(opportunity)
    
    def _create_fallback_proposal(self, opportunity: Dict[str, Any]) -> Dict[str, Any]:
        """Create a fallback proposal when AI generation fails."""
        title = opportunity.get('title', 'System Improvement')
        opp_type = opportunity.get('type', 'optimization')
        
        # Create intelligent fallback based on opportunity type
        if opp_type == 'optimization':
            return {
                "title": f"Optimize {title}",
                "description": f"Implement performance improvements for {title.lower()}",
                "rationale": f"This optimization will improve system performance and efficiency",
                "target_files": opportunity.get('target_files', ['core/ai_engine.py']),
                "estimated_impact": {
                    "performance": "20% improvement",
                    "maintainability": "10% improvement",
                    "reliability": "15% improvement"
                },
                "risk_level": "LOW",
                "priority": opportunity.get('priority', 'MEDIUM')
            }
        elif opp_type == 'bug_fix':
            return {
                "title": f"Fix {title}",
                "description": f"Resolve issues related to {title.lower()}",
                "rationale": f"This fix will improve system stability and reliability",
                "target_files": opportunity.get('target_files', ['core/ai_engine.py']),
                "estimated_impact": {
                    "performance": "10% improvement",
                    "maintainability": "20% improvement",
                    "reliability": "25% improvement"
                },
                "risk_level": "LOW",
                "priority": opportunity.get('priority', 'HIGH')
            }
        elif opp_type == 'refactoring':
            return {
                "title": f"Refactor {title}",
                "description": f"Improve code structure and maintainability for {title.lower()}",
                "rationale": f"This refactoring will improve code quality and maintainability",
                "target_files": opportunity.get('target_files', ['core/ai_engine.py']),
                "estimated_impact": {
                    "performance": "5% improvement",
                    "maintainability": "30% improvement",
                    "reliability": "10% improvement"
                },
                "risk_level": "MEDIUM",
                "priority": opportunity.get('priority', 'MEDIUM')
            }
        else:
            return {
                "title": f"Improve {title}",
                "description": f"General improvement for {title.lower()}",
                "rationale": f"This improvement will enhance overall system performance",
                "target_files": opportunity.get('target_files', ['core/ai_engine.py']),
                "estimated_impact": {
                    "performance": "15% improvement",
                    "maintainability": "15% improvement",
                    "reliability": "15% improvement"
                },
                "risk_level": "LOW",
                "priority": opportunity.get('priority', 'MEDIUM')
            }
    
    async def _run_tests_for_changes(self, target_files: List[str]) -> Dict[str, Any]:
        """Run tests for the modified files."""
        try:
            # This would run actual tests
            # For now, return success
            return {
                "success": True,
                "tests_run": 15,
                "tests_passed": 15,
                "tests_failed": 0,
                "coverage": 0.85
            }
            
        except Exception as e:
            logger.error(f"Error running tests: {str(e)}")
            return {"success": False, "error": str(e)}
    
    async def _rollback_changes(self, code_changes: Dict[str, str]):
        """Rollback applied code changes."""
        try:
            # This would implement actual rollback logic
            logger.info("Rolling back code changes...")
            
        except Exception as e:
            logger.error(f"Error rolling back changes: {str(e)}")
    
    async def _learn_from_improvement(self, proposal: SelfImprovementProposal):
        """Learn from successful improvements to improve future proposals."""
        try:
            # Record learning data
            learning_entry = {
                "proposal_id": proposal.id,
                "improvement_type": proposal.description,
                "success": True,
                "impact": proposal.estimated_impact,
                "timestamp": datetime.now().isoformat()
            }
            
            if "successful_improvements" not in self.learning_data:
                self.learning_data["successful_improvements"] = []
            
            self.learning_data["successful_improvements"].append(learning_entry)
            
            logger.info(f"Recorded learning from successful improvement: {proposal.id}")
            
        except Exception as e:
            logger.error(f"Error learning from improvement: {str(e)}")
    
    async def _record_version_change(self, proposal: SelfImprovementProposal, implementation_result: Dict[str, Any]):
        """Record a version change for the implemented proposal."""
        try:
            # Determine change type based on proposal description
            change_type = self._determine_change_type(proposal.description)
            
            # Determine impact level based on proposal risk level
            impact_level = self._determine_impact_level(proposal.risk_level)
            
            # Calculate lines changed
            lines_added = 0
            lines_removed = 0
            for diff in proposal.code_changes.values():
                diff_lines = diff.split('\n')
                lines_added += len([line for line in diff_lines if line.startswith('+') and not line.startswith('+++')])
                lines_removed += len([line for line in diff_lines if line.startswith('-') and not line.startswith('---')])
            
            # Record the improvement
            change_id = await self.version_tracker.record_improvement(
                proposal_id=proposal.id,
                change_type=change_type,
                title=proposal.title,
                description=proposal.description,
                files_modified=proposal.target_files,
                lines_added=lines_added,
                lines_removed=lines_removed,
                impact_level=impact_level,
                implemented_by=proposal.generated_by,
                approved_by="CEO"  # This would come from approval engine
            )
            
            # Finalize the improvement
            await self.version_tracker.finalize_improvement(change_id, success=True)
            
            logger.info(f"Recorded version change: {change_id}")
            
        except Exception as e:
            logger.error(f"Error recording version change: {str(e)}")
    
    def _create_intelligent_fallback_analysis(self, performance_metrics: Dict[str, Any], 
                                            code_quality: Dict[str, Any], 
                                            technical_debt: Dict[str, Any]) -> Dict[str, Any]:
        """Create intelligent fallback analysis based on collected metrics."""
        issues = []
        opportunities = []
        
        # Analyze performance metrics
        cpu_usage = performance_metrics.get('cpu_usage', 0)
        memory_usage = performance_metrics.get('memory_usage', 0)
        error_rate = performance_metrics.get('error_rate', 0)
        response_time = performance_metrics.get('response_time_avg', 0)
        
        if cpu_usage > 70:
            issues.append({
                "type": "performance",
                "title": "High CPU Usage",
                "description": f"System CPU usage is {cpu_usage}%, indicating potential performance bottlenecks",
                "severity": "MEDIUM",
                "affected_components": ["ai_engine", "data_processor"]
            })
            opportunities.append({
                "type": "optimization",
                "title": "Optimize CPU-Intensive Operations",
                "description": "Implement caching, parallel processing, and algorithm optimization",
                "priority": "HIGH",
                "estimated_impact": {
                    "performance_improvement": "30%",
                    "code_quality_improvement": "15%",
                    "maintenance_improvement": "20%"
                },
                "target_files": ["core/ai_engine.py", "utils/data_processor.py"],
                "risk_level": "LOW"
            })
        
        if memory_usage > 80:
            issues.append({
                "type": "performance",
                "title": "High Memory Usage",
                "description": f"System memory usage is {memory_usage}%, may cause performance issues",
                "severity": "MEDIUM",
                "affected_components": ["data_manager", "cache_system"]
            })
        
        if error_rate > 0.05:
            issues.append({
                "type": "reliability",
                "title": "Elevated Error Rate",
                "description": f"Error rate of {error_rate}% indicates system instability",
                "severity": "HIGH",
                "affected_components": ["all_components"]
            })
            opportunities.append({
                "type": "bug_fix",
                "title": "Improve Error Handling and Recovery",
                "description": "Enhance error handling, add retry mechanisms, and improve logging",
                "priority": "CRITICAL",
                "estimated_impact": {
                    "performance_improvement": "10%",
                    "code_quality_improvement": "25%",
                    "maintenance_improvement": "30%"
                },
                "target_files": ["core/ai_engine.py", "utils/helpers.py", "monitoring/metrics.py"],
                "risk_level": "LOW"
            })
        
        # Analyze code quality
        test_coverage = code_quality.get('test_coverage', 0)
        documentation_coverage = code_quality.get('documentation_coverage', 0)
        code_complexity = code_quality.get('code_complexity', 0)
        
        if test_coverage < 0.8:
            issues.append({
                "type": "code_quality",
                "title": "Low Test Coverage",
                "description": f"Test coverage is {test_coverage*100}%, below recommended 80%",
                "severity": "MEDIUM",
                "affected_components": ["all_components"]
            })
            opportunities.append({
                "type": "feature",
                "title": "Increase Test Coverage",
                "description": "Add comprehensive unit and integration tests",
                "priority": "HIGH",
                "estimated_impact": {
                    "performance_improvement": "5%",
                    "code_quality_improvement": "40%",
                    "maintenance_improvement": "35%"
                },
                "target_files": ["tests/"],
                "risk_level": "LOW"
            })
        
        if documentation_coverage < 0.7:
            issues.append({
                "type": "maintainability",
                "title": "Insufficient Documentation",
                "description": f"Documentation coverage is {documentation_coverage*100}%, needs improvement",
                "severity": "LOW",
                "affected_components": ["all_components"]
            })
        
        # Analyze technical debt
        total_todos = technical_debt.get('total_todos', 0)
        deprecated_functions = technical_debt.get('deprecated_functions', 0)
        unused_imports = technical_debt.get('unused_imports', 0)
        
        if total_todos > 10:
            issues.append({
                "type": "maintainability",
                "title": "High Technical Debt",
                "description": f"{total_todos} TODO items indicate accumulated technical debt",
                "severity": "MEDIUM",
                "affected_components": ["all_components"]
            })
            opportunities.append({
                "type": "refactoring",
                "title": "Address Technical Debt",
                "description": "Systematically address TODO items and technical debt",
                "priority": "MEDIUM",
                "estimated_impact": {
                    "performance_improvement": "10%",
                    "code_quality_improvement": "30%",
                    "maintenance_improvement": "40%"
                },
                "target_files": ["all_files"],
                "risk_level": "MEDIUM"
            })
        
        if deprecated_functions > 0:
            issues.append({
                "type": "maintainability",
                "title": "Deprecated Functions",
                "description": f"{deprecated_functions} deprecated functions need to be updated or removed",
                "severity": "LOW",
                "affected_components": ["utils", "core"]
            })
        
        # Calculate health score based on issues
        health_score = 0.85  # Base score
        if len(issues) > 0:
            high_severity = len([i for i in issues if i.get('severity') == 'HIGH'])
            medium_severity = len([i for i in issues if i.get('severity') == 'MEDIUM'])
            health_score -= (high_severity * 0.1) + (medium_severity * 0.05)
        health_score = max(0.3, min(0.95, health_score))  # Clamp between 0.3 and 0.95
        
        return {
            "health_score": health_score,
            "issues": issues,
            "opportunities": opportunities
        }
    
    def _determine_change_type(self, description: str) -> ChangeType:
        """Determine the change type based on description."""
        description_lower = description.lower()
        
        if any(word in description_lower for word in ["bug", "fix", "error", "issue"]):
            return ChangeType.BUG_FIX
        elif any(word in description_lower for word in ["performance", "speed", "optimization"]):
            return ChangeType.PERFORMANCE_IMPROVEMENT
        elif any(word in description_lower for word in ["feature", "add", "new"]):
            return ChangeType.FEATURE_ADDITION
        elif any(word in description_lower for word in ["refactor", "restructure", "clean"]):
            return ChangeType.CODE_REFACTORING
        elif any(word in description_lower for word in ["security", "vulnerability"]):
            return ChangeType.SECURITY_UPDATE
        elif any(word in description_lower for word in ["documentation", "docs", "readme"]):
            return ChangeType.DOCUMENTATION_UPDATE
        elif any(word in description_lower for word in ["architecture", "design"]):
            return ChangeType.ARCHITECTURE_CHANGE
        else:
            return ChangeType.OPTIMIZATION
    
    def _determine_impact_level(self, risk_level: str) -> ImpactLevel:
        """Determine impact level based on risk level."""
        risk_mapping = {
            "LOW": ImpactLevel.LOW,
            "MEDIUM": ImpactLevel.MEDIUM,
            "HIGH": ImpactLevel.HIGH,
            "CRITICAL": ImpactLevel.CRITICAL
        }
        return risk_mapping.get(risk_level, ImpactLevel.MEDIUM) 